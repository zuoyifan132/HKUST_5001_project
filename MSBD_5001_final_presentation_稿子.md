For the neural network, we use the multi-layer perceptron classifier, we tune the hyper parameters to achieve the best performance. Here is the hyperparameters that we used. we use the Adam solver, the network has 6 perceptrons layer followed by 3 perceptrons layer. we use Relu as our activation function. and we set the maximum iteration to be 1000. and alpha to be 10 to the negative 4. Our model could achieve 90.1% accuracy. Next, in order to ranking the feature importance, we introduce concept of permutation feature importance from the interpretable machine learning. It measures the incerease in the prediction error of the model after we permuted the features's value which breaks the relationship between the feature and the true outcome. The theory state that a feature is more important if shuffling its value increase the model error and vice versa. the following is how the algorithm works, we firsly compute the original model error, then for each feature we randomly shuffle its column and fix the other columns. We calculate the error corresponding to each features and then we do a little bit transformation to make the result more clear. Lastly, we ranking the error by descending order. Note that here we use Mean square error for calculation. Here is the code which shows how we calculate the error. Note that for each feature error, we loop it for 10 times and summing up and averaging the error to make the result more stable. The transformation shows that how much percent each feature error exceed the original error. Remember that a feature is consider more siginificant if the error exceed the orginal error a lot compare to the others. Here is the ranking of feature importance. The result shows that the most siginificant feature is pagevalue which the error exceed the original error by around 60.7% which is a lot. The top five siginificant features for this model with permutation feature importance  are pagevalue, the month of November, information duration, month of december and administrative duration. 

